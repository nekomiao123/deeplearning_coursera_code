# Batch normalization

## batch norm实现

在z和a中间，对z进行归一化，归一化之后均值为0 方差为1 

$\mu = \frac{1}{m} \sum_{i}z^{i}$ 

$\sigma^{2} = \frac{1}{m}\sum_{i}(z^{i}-\mu)$

$z_{norm}^{i}=\frac{z^{i}-\mu}{\sqrt{\sigma^{2}+\epsilon}}$

$z^{i}=\gamma z_{norm}^{i}+\beta$

加入两个参数$\gamma$和$\beta$来改变归一化后 均值为0和方差为1 的情况，这两个参数也需要调，可以用梯度下降

## 用mini-batch 与batch norm

让我们总结一下关于如何用Batch Norm来应用梯度下降法，假设你在使用Mini-Batch梯度下降法：

- 你运行 *t = 1* 到batch数量的for循环，你会在Mini-Batch *X t* 上应用正向prop，每个隐藏层都应用正向prop，用Batch Norm代替 *z[l]* 为 *Ž[l]*。
- 接下来，它确保在这个Mini-Batch中， *z* 值有归一化的均值和方差，归一化均值和方差后是 *Ž[l]*
- 然后，用反向prop计算 *dw[l]* 和 *db[l]* ，及所有l层所有的参数， *dβ[l]* 和 *dγ[l]* 。尽管严格来说，因为你要去掉 *b* ，这部分其实已经去掉了。归一化之后常数项不起作用了（你要计算 *z[l]* 的均值，再减去平均值，在此例中的Mini-Batch中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减去所抵消）
- 最后，你更新这些参数：
  - $w^{[l]} = w^{[l]} - αdw^{[l]}$
  - $β^{[l]} = β^{[l]} - αdβ^{[l]}$
  - $γ^{[l]} = γ^{[l]} - αdγ^{[l]}$

## batch norm起效的原因

- 归一化之后加速学习
- 它可以使权重比你的网络更滞后或更深层（让值的改变更小，每层更加独立。Batch Norm减少了输入值改变的问题，它的确使这些值变得更稳定，神经网络的之后层就会有更坚实的基础）
- 它有轻微的正则化效果（在使用mini-batch的时候，和dropout相似，它往每个隐藏层的激活值上增加了噪音）

## 测试时的Batch Norm

理论上你可以在最终的网络中运行整个训练集来得到 μ 和 σ2 ，但在实际操作中，我们通常运用指数加权平均来追踪在训练过程中你看到的 μ和 σ2 的值

