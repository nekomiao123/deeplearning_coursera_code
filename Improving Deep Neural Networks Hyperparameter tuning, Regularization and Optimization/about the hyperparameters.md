# Hyperparameter tuning

$\alpha$ (learning rate) is the most important parameter  

Then the second is 

- Momentum——the $\beta$ parameter. 0.9 is a good default value
- Mini-batch. It can be setted from 64 to 512
- The hidden units

The third is 

- the number of layers
- Learning rate decay
- When we use Adam, $\beta_{1}$ = 0.9 $\beta_{2}$ = 0.999 and  $\epsilon$ = $10^{-8}$

